{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jax.GD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahuldave/LearningJax/blob/main/Jax_GD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlHg1tBH7eiy"
      },
      "source": [
        "# solving y=mx+b on a TPU\n",
        "\n",
        "_part of the [solving y=mx+b... with jax on a tpu pod slice](http://matpalm.com/blog/ymxb_pod_slice/) blog series_\n",
        "\n",
        "in the last colab we introduced some key jax transforms. \n",
        "\n",
        "in this colab we'll use them to fit a simple `y=mx+b` model using squared loss; both on a single device as well as across 8 devices in a data parallelism approach.\n",
        "\n",
        "while the approach described here is clearly overkill for such a trivial model it's good to work through something simple to focus on the tooling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDonUX0td8on",
        "outputId": "9e50e94d-b55a-43f9-e10c-c6fc5042dc70"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvx44mUvnzY0"
      },
      "source": [
        "setup stuff for runtime TPU or CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjkvme7eYYZx"
      },
      "source": [
        "USE_TPU = False\n",
        "\n",
        "if USE_TPU:\n",
        "  import jax\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "  # x8 cpu devices  \n",
        "  import os\n",
        "  os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=2'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvJbOLW4YccB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4bb9d3-89f2-4937-8743-1373ee1b3112"
      },
      "source": [
        "import jax\n",
        "jax.devices()\n",
        "num_replicas = len(jax.devices())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YFomemlh01O"
      },
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "from jax import make_jaxpr"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFPE54T4rGDL"
      },
      "source": [
        "we start by generating training data for a simple line with a bit of noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cWNrO6_-50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "57605809-0e65-4a89-9e72-b06dce4912fd"
      },
      "source": [
        "def y(x, m, b):\n",
        "  return m * x + b\n",
        "\n",
        "datasize = 128\n",
        "x = np.linspace(-10, 10, num=datasize)\n",
        "np.random.shuffle(x)\n",
        "\n",
        "m_true = -1.5\n",
        "b_true = 2.0\n",
        "\n",
        "y_true = y(x, m_true, b_true)\n",
        "\n",
        "noise = ((np.random.rand(*y_true.shape) * 2) - 1) * 3\n",
        "y_true += noise\n",
        "\n",
        "plt.plot(x, y_true, 'o')\n",
        "plt.vlines(0, -12, 12)\n",
        "plt.hlines(0, -12, 12)\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD7CAYAAACPDORaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAezUlEQVR4nO3dfYxc5XUG8OfZ8eCMIWWdsHXMBBcnorihCG9ZkUROI9tJMflkcRogbSOqojqRglosYnWdVAGSSKziEio1H5WjoFClBDsFjIkRhmC7qJbSsK7XGIOtEL7CxMEbYCHBgz3ePf1j5tqzs/dz7r3zcef5SZZ379yde4c1Z94573nPSzODiIhkU1+7b0BERNKjIC8ikmEK8iIiGaYgLyKSYQryIiIZpiAvIpJhsYM8yXNI7iT5JMkDJP+xdvxtJB8m+Yva3/Pj366IiETBuHXyJBcCWGhm/0fyrQD2ABgG8LcAXjGzUZIjAOab2T/FvWEREQkvdpCf9YTkfQC+Vfuz3MwO194IdpnZ+X4/e9ZZZ9m5556b6P2IiGTdnj17fmtmA26PzUnyQiTPBTAI4H8BLDCzw7WHfgNggcfPrAGwBgAWLVqEsbGxJG9JRCTzSD7v9VhiE68kzwBwN4Drzez1+ses+nHB9SODmW00syEzGxoYcH0jEhGRJiUS5EnmUQ3w/2lm99QOv1RL0zh5+yNJXEtERMJLorqGAL4P4Ckz+2bdQ1sBXFP7+hoA98W9loiIRJNETn4ZgM8C2E9yvHbsSwBGAWwmeS2A5wFcmcC1REQkgthB3sz+BwA9Hv5Q3OcXEZHmJVpdI6ds2VvChu2H8OvJMs7uL2DdqvMxPFhs922JSI9RkE/Blr0lrL9nP8qVKQBAabKM9ffsBwAFehFpKfWuScGG7YdOBnhHuTKFDdsPtemORKRXKcin4NeT5UjHRUTSoiCfgrP7C67H+0hs2Vtq8d2ISC9TkE/BulXno5DPzTo+ZYb19+xXoBeRllGQT8HwYBG3rL4QOc6uLFVuXkRaSdU1CXErmZz26PCp3LyItIqCfAK8Sib75+Xx6tHKrPO9cvYiIklTuiYBXiWTZpiVmy/kc1i3yretvohIYhTkE+CVfnmtXMEtqy9Esb8AAij2F3DL6gu1IEpEWkbpmojccu9n9xdQcgn0Z/cXMDxYVFAXkbbRSD4CJ/demizDcCr3vmLJgNIyItKRFOQj8Mq97zw4obSMiHQkpWsi8GtX0GxaJqhbpbpZikgcCvIR+OXemxHUrVLdLEUkLqVrInBrVxAn9x7UrdLr8Rs271NrBBEJRSP5CJzRc1Lpk6BulV6POz1w6u9JRMSNgnxESZZEBqV/vB4HTo34FeRFxI/SNW0UlP7x6mbpUA8cEQmikXwbBaV/nL9v2LwPUy7NztQDR0SCJBLkSd4O4OMAjpjZn9aO3QTg7wFM1E77kpk9kMT1siQo/eM8Vl9lA2ixlYiEk9RI/gcAvgXgPxqO32Zm/5LQNdqqsV59xZIB7Dw40ZL69aQnfEWkdyQS5M3sUZLnJvFcncitXv2HP3vh5OOtqF9XDxwRaUbaE6/XkXyc5O0k57udQHINyTGSYxMTE26ntJ1bvXoj7fgkIp0ozSD/XQDvBrAUwGEAt7qdZGYbzWzIzIYGBgZSvJ3mha1iUbWLiHSa1KprzOwl52uS3wPwk7SulTa/evXG8zq110yn3peIpCu1IE9yoZkdrn17BYAn0rpW2tatOn9WdUujQj6HFUsGPHvNAO2bOFUPHJHeRfPYbDrSk5A/ArAcwFkAXgJwY+37pQAMwHMAPlcX9F0NDQ3Z2NhY7PtJQ5jqmg3bD7mO+OfPy+PNyvSMN4l8H3HGW+Zg8mgl9aC/bHSH630V+wvYPbIylWuKSOuQ3GNmQ26PJVVd8xmXw99P4rk7RZjqlrWbxl2Pu23mXZm2k8eTGFn7pWOCeuSISHZpxWtM9cG1j3RdmRpGnF40QemYpFski0j3UO+aGBq3A3QL8IV8Dv2FfKjna3ZkHdSyOOkWySLSPTSSj8Grfj5HYtrsZNoEmN2WwE2zI+ugdIxWzIr0LgX5GLyC67QZnh392KzjTpA9s5DHG8dPoDJ1auQfZ2QdJh2jFbMivUnpmhi8Rt5ux4cHi9g9shK3XbUUp8+dg8qUIUcCcN/4e8veEpaN7sDikW1YNrrDdycopWNExIuCfAxRg2t9Dh+o5vCd8xsDfH2u35lIdQv0zsRvuTLl+6YhIr1JQT6G4cEibll9IYr9BRDBwTVogjTqeWHfNESkdyknH1OUXHfYevWw5/m9GSjIiwigkXxLhc3hhz1Pi5xEJIiCfAuFzeGHPS/KxK+I9CYF+RYKk8OPMpGqqhoRCaKcfIv55fAb2xMETaQ2s8ipndsYikjrJdKFMimd3IUybVv2lnDD5n2urRGS6hbZ+CbippDPqfzSxfLlywEAu3btaut9iLhJvQulxOMEX6/mZm4TqUGbgLg9HmUbQwV5kWxQkO8AQcG3cSI1qOuk1+NBAd6h6hyR7NDEawfwC6puE6lBi6W8HncmcoOoOkckOxTkO4BXUM2RrvnxoPp4r8ediVw/+RzxxrEToXrmiEjnU5DvAF6lkLdeeZFrbjyoPt7rcacUs76E82/et+jk9/Pn5QEDJsuVwJ45ItIdlJPvAFFLId02Fq9P6/g97lfCuWx0x6ytCjURK9LdFOQ7RJQeOEFvCs1uEqI2CSLZk0i6huTtJI+QfKLu2NtIPkzyF7W/5ydxrV7n9Jl3Ng2/7aql2D2yclYAr+9fD1Q3GQ/KsatNgkj2JJWT/wGAyxqOjQB4xMzOA/BI7XuJIUqf+WbOb6ZNQpTNTUSk9RIJ8mb2KIBXGg5fDuCO2td3ABhO4lq9LGyf+WbPj9ofP+qbiIi0Xpo5+QVmdrj29W8ALEjxWj0has68mRx7lLkB9bMX6XwtKaG0aoMc1zX7JNeQHCM5NjEx0Yrb6VpRc+Zp59g1USvS+dIM8i+RXAgAtb+PuJ1kZhvNbMjMhgYGBlK8ne4XNWeeditirzeLPlI5epEOkWaQ3wrgmtrX1wC4L8Vr9YSw/eididAN2w/hUxcXQ+fYo3J7EwGqK2uVoxfpDInk5En+CMByAGeRfBHAjQBGAWwmeS2A5wFcmcS1el2UfvSlyTLu3lNKrXVwYz1+Hzmrk6Zy9CLtlUiQN7PPeDz0oSSeX8Jpx0Ro/ZvO4pFtrueUJstYPLJNm5KItIF612RIuydC/SZ0nfTN9ZvGsfTmh5TCEWkRBfkMafeKVa8cfaPJckW5epEWUZDPkHZv7N04MezHb1GWiCRHDcoypNnGZH6Cthl0uwfn8WWjO1DySRWVJsvYsrekHL1IirSRd48KE7zdNv722ujb7fkABG47mO8jznjLHEwerXT0xKw28pZO5reRt4J8D3IL3vkccfppc/BauTJj42+3kXixv4DdIyt9n895MwCAm+8/MKtPvRevN5F2U5CXTuYX5JWT70FupZaVKZuxI9S6H+/zTLU0VusElW7u/cql+Nday+MgytWLJEtBvgeFKamsTHt/wusjZ1TGhCndHB6srrxN6v5EJBwF+R4Ut6RyymxGCWTY0s2wJZbapEQkOQryPShssPVTrkzhhs37sHhkG44eP4F838yiSbfSzcYSy/5CHvlc8M+JSPNUQtmDGkst++fl8fs3T/imaNw4fWpePVpBPkf0F/IzJm7dJk8be+80VuWsWDKADdsPYe2m8Y6uthHpFgryPcov2J5ZyOON4ydQmQof9CtThtPnzsH4jZc2fR9uDdbW37P/5HkiEp2CvADwD/phR/pxJ0y105RI8hTkxZVb0L9h875ZrYTr1U+YRl0pC7S/wZpIFmniVUIZHixi2ifA10+YNrvBd7sbrIlkkYK8hOYVbHPkjFWqfmkXP+1osFa/k5a2K5QsUrpGQlu36vxQvWyaTbuk0WDNy5a9Jdy09QAmy6faLWiiV7JIQV5CCxuEz+4vuLZECJN28dveMCluvXYcmuiVrFGQl0jCBGGvEX+SaZdmJnYdbumkeprolSxRkJfEpZ12iVtPHxTENdErWaIgL6mImnaJMjKPW0/vlU4CTn3iaLyfY29fgjNePhj69Yh0itSDPMnnAPwOwBSAE149j6W3+C22qh+ZA7M/EcStp3dLJwHA/Hl53PiJCwBg1icFvuuyyK9RpBO0aiS/wsx+26JrSYdrTLe4bShSrkzh5vsP4M3K9Ky0TP+8vOvPhE2zBKWTlo3umPUGYLk8Xl30wfAvUqRDKF0jLRc08enwCv5z5/ShkM/Fmtj1Syd5fSKYOu0PQj+/SKdoxWIoA/AQyT0k1zQ+SHINyTGSYxMTEy24HWm3uNUrr5UrM1oWF/sLsbcMrF8U1Ue6npM7/nrTzy/SLq0I8h8wsz8D8BEAXyA54zOvmW00syEzGxoYGGjB7Ui7xa1e6SOxdtM4AOCv37cIALB203jTK1Yb2zC49efhVAXzX3g01n2LtEPq6RozK9X+PkLyXgCXAND/LT3MbeKzfiPxoAbHThAuTZbxw5+9cPJ4mFJKtyoer/RRjsS0WbW65rH7VV0jXSnVIE/ydAB9Zva72teXAvhqmteUzhdm4tOrxDGIXymlV3291/zAtBmeHf0YAGD5cv2zle6U9kh+AYB7Wc1xzgFwp5k9mPI1pQv4TXx6rZgNM1kLVIP3lr2lWc/vVV+fI11TNFoUJVmQapA3s2cAXJTmNSR7vEb6G7YfCj3Cd0vbeFbNmMWu1hHpVCqhlI7kNdL3S6/UczYar98r1mula7HuTSTt7pciraYgL13DbYS/YsnAjMnXevUTtOvv2Y9PXVzE3XtKriP2VnS/FGkHBXnpKm7BeOfBicA0TrkyhZ0HJ3DL6gs1YpeeoiAvXcGvgZlXL5pGzhvB7pGVqd+vSKdQkJeOF9Ra2An2jTs9udHOT9JrtMerdLwwe8YODxZx+tzgMUuYvWb9aE9Y6TYayUvHC9taOGxPnCi9c5w0Uem9X0TfiTLW/Xifa0tkfTKQTqUgLx0v7J6xfpuB+P1cPc8+9ySm8/MwPT1z0ZRTqgm4B/o42xSKJEHpGul461adj0I+N+OY22Ilt/Ma+S1yamxU9urRyslRu58pM1y/aRxLb35oRvqm8fmckb9SPNJKGslLxwu7Z6xXHf3OgxNNbysYxWS5MiN94zWX4DfyF0kazaVnR7sMDQ3Z2NhYu29DMihM2mTxyLbADphhFPsL2D2y0vf5Cvlc7B74Ig6Se7y2VlW6RjIvbNokqYZkzsSu3/PFrfIRCUtBXjIvTAkm4J7Tz+fcd4ny4wT3oDkCp1umSJoU5CXzwpZgDg8WZ20ruOEvL0LRY0Te5xL/6yd2nefLeWwnCEATsZI6TbxK5oUtwQS8u1+uvfMxWC5/8nsnpw74Twg7X3u1XfDb5EQkCRrJS+aFLcH0MjxYxNufeRC5Y6/N2jh8eLCI3SMrcdtVSwG47zXrjOi9xN3YXMSPRvKSeWFLMP2c8fJBnPHyQezatWvWY0G9dZy/vTY90Q5UkiYFeekJafaL95vYrb+m17aG2oFK0qQgLxJTlIldIN4nCpGoFORFYkpiYlc9biQtCvIiMTWThvFshAZ1t5RkpV5dQ/IykodIPk1yJO3ribSaW329X8uCMI3QtCJWkpLqSJ5kDsC3AfwFgBcBPEZyq5k9meZ1RVotysRu2EZopckyFo9sU/pGYkk7XXMJgKfN7BkAIHkXgMsBpBLkly9fnsbTimB8fBxAMv/GSu/9IuCzCrae02tn7Z2P4etf/xrOePlg7OtLZ3Irz01C2umaIoBf1X3/Yu3YSSTXkBwjOTYxMZHy7Yi0X+7465F/xnJ5vLrogyncjWRd2ydezWwjgI1AtdVwnOdK651QxBnBJ/FvrHHxFFBthHZi2uDX+Xt67pn6Ny6RpT2SLwE4p+77d9aOifQsr0ZoQc3sm1kZq43HJe2R/GMAziO5GNXgfjWAv0r5miIdz22i1qvtAdDcytgw7RYk+1IdyZvZCQDXAdgO4CkAm83sQJrXFOlWXv3n+wv5pnaRCttHX7It9Zy8mT0A4IG0ryPS7ZJuexC23YJkW9snXkXklMY0jpNTbyboR2m3INmlfvIiKUhiwjPs3rRe4vbRl2zQSF4kQVv2lnDz/Qfw6tHKyWN+E55+jcnCtjD2oq6XAijIiyTGrf7d4Racg6pfwuTUg7pXptlHX7qDgrxIQoJ60jQG7aCRuldOvY/EP2/Zj22PHw79iUF6l3LyIgkJqlppnPAMGql7lVROmeGHP3thRoB3qERSGmkkL5IQr5E34D7hGVT94ozGb9i8D1N+/Q4aJFkiqc1Mup9G8iIJibqYKUz1y/BgEdMRAjyQXIlk3Ooe6QwayYskJGo1S9jz/T4hNGq2RNJtxB63ukc6g4K8SIKiVrOEOd9te0E3/YU8bvrkBZEDsFeVj9f1tGK2uyjIi3Q4txH/iiUD2HlwIpFcudeIPUe6zgVoxWx3UZAX6QJp1rt7jcynzFDI52b2ve8jjh4/oW0Ju4gmXkV6nNfI3NmQ3Ol731/IA6xuPK6J2O6hIC+SIc30zPGr8hkeLGL3yEo8O/oxnD53DipTM9M3qsvvfErXiGREs5uEhK3yUevi7qQgL9JF0mpoFibnr9bF3UlBXqRL+I3Une/dJDXSdivlVOvizqcgL9IlvEbqN99/AG9Wpj1/rtkNwP0+MajNQfdQkBfpEl4jcrdGZY40NgBXUO8uqq4R6RLNjMidnHyUMsdmNgBPYicsSUdqQZ7kTSRLJMdrfz6a1rVEeoFXqWN/Ie/7c1Hr2aNW0bg1Mlv3430Y/OpDCvodIO2R/G1mtrT254GUryWSacODxRmLk5zFSjd98gLX7pf1otSze31i8DruNvKvTJsWTXUI5eRFuohfTtyZEPVqTPzryXKo/vBRq2jCVO+oe2X7pD2Sv47k4yRvJznf7QSSa0iOkRybmJhI+XZEsql+ZWrRY8TdPy8f2B/eeRNwGpQBpz4xeAXosHMFWjTVHrGCPMmfknzC5c/lAL4L4N0AlgI4DOBWt+cws41mNmRmQwMDA3FuR0Tgnbs3g++Ean1uHTjVoCyoTNJrs5RGWjTVHrHSNWb24TDnkfwegJ/EuZaIhONVz75207jr+c4Iu9kVs43XO7OQxxvHT8zoc6NFU+2TWk6e5EIzO1z79goAT6R1LRGZyS13v2H7IddVsQbg3JFtns8VJs3SeL2g3L/2jm2dNCdev0FyKar/hp4D8LkUryUiAcLuMNWomTSL3wRxs43UpDmpBXkz+2xazy0i0dRPqEaRRpolSlpII/74VEIpknGNI+cwCKQWVMMuttKIPxkK8iIZF3UEX+wvYPfIytTuJ2zL4jitk+UUBXmRjItSn550esYt3RJ2sZU2KUmGGpSJZJzXxKmz2Cnsoqeo3HraOOkWt/YMjdeN2l5B3GkkL5JxXiPnOAE9zISoX7pl98jKwGv7jfg1IRuegrxIxiW92UfYCdG46Rav+wagCdkIFORFekCSm32EnRANM8EaNCJ3u+9lozs0IRuBcvIiEknYEbpXDx1nNO6Vsw9qSawJ2WgU5EUkkrATol797+vTMFF3oIpyfalSukZEAtWnVc4s5JHP0bUBmVv6xavmvtkRedR+971OQV5EfDVOtE6WK8j3EfPn5fHq0QpyJMqVKdx8/wH8/s0TqExXg3/QhGjYRVGNkp5IzjoFeRHx5bW9n1l1BO089urRyqyf9ZsQjTMiT3IiOesU5EXEl1f6ZLI8O6hH+XmNyFtDQV5EfHmlVaL8vFeppNeIPKnFTlo0pSAvIgG80ipvyfe5pmjqFfI5rFgyEGnxUpTuk35BXF0sq1RCKSK+vEohb/zEBbPq4PM5or+Qn3HezoMTkUolw5ZWBtXZN1uimTUayYtIIL+JzqB0SNDess0eD1p5q0VTVQryItK0MFUuUUslw54fFMSbLdHMGqVrRCRVQe0Nmj0/aOWr2/MAwBvHTgS2TsgSBXkRSVVQe4Mw53/q4iI2bD+ExSPbsGx0B7bsLQW+GTjPM39efsY5k+VKqB45WUEzCz6rRYaGhmxsbKzdtyEyy/LlywEAu3btaut99CKvPWr7C3l8/KKF2HlwwndOYNnoDte0TdrbHLYSyT1mNuT2WKycPMlPA7gJwJ8AuMTMxuoeWw/gWgBTAP7BzLbHuZaI9CavPWonyxXcvacUuPlJr0/Axk3XPAFgNYBH6w+SfA+AqwFcAOAyAN8hOTs5JiKC6mh92eiOGekYh18wjtO1so/siZRNrCBvZk+Zmdt/4csB3GVmx8zsWQBPA7gkzrVEJJuC6t2DqmHCdK10m4CdMuuJ3HxaE69FAL+q+/7F2rFZSK4hOUZybGJiIqXbEZFOFbRoyStIO8J0rbxl9YUnNyz3uk5WBebkSf4UwDtcHvqymd0X9wbMbCOAjUB14jXu84lI53JrQxCUM3fy7Tfff2BWG4UoXSujLsrKisAgb2YfbuJ5SwDOqfv+nbVjItKjvHrJ9Nf60jeqH6E7i67iNBzr1cVRaa143QrgTpLfBHA2gPMA/Dyla4lIF/BKy8yd0zejLz3gPUKP00c+Sv/6LHWvjJWTJ3kFyRcBvB/ANpLbAcDMDgDYDOBJAA8C+IKZza6BEpGe4ZUWea1cibRYqllhF2U1u8F4p9JiKJEQtBgqvjCLkjphBN2Ni6f8FkOprYGItERQG4JOGUFnbfGUgryItERQuqRT+r8HNT7rNmo1LCIt4zdx2s4RdH2a6MxCHvkcUZk6lcoOW6rZiTSSF5GO0K4RdGOaaLJcAQyYPy+f6kRwq2gkLyIdIUqJY5Lc0kSVacO80+Zg71cuTfXaraCRvIh0hKh955PilQ4qTZa7tmyynkbyIiGodLI14ix2apbXSlgAWH/PfgDo2lQNoJG8iPQ4vwZoWWhgppG8iPQ0Z5R+fUYbmGkkLyKZ47cJiZvhwSKKGauPdyjIi0imNLtyNmhFbrdSukZEup6zmMlrAtXJrftNoNavvM1C90mHgryIdLXGPvVewuTW21HdkzYFeRHpam6LmdzU59Y7odtlqyjIi0hXCzNCd+t22bhDFdDd9fBeNPEqIl0tqPolR3Zkt8tWUZAXka7mt5ipkM/h1isvmjFCz1q/+CAK8iLS1ep73gDVkTvg3fsma/3igygnLyJdL0pVTK9t6K0gLyI9JWw9fFYmaJWuEZGeMzxYxO6RlbjtqqUAgLWbxme1P8jKBG2sIE/y0yQPkJwmOVR3/FySZZLjtT//Hv9WRUSSE9T+ICsTtHFH8k8AWA3gUZfHfmlmS2t/Ph/zOiIiiQoaqWdlgjZWkDezp8ysuz67iIggeKSelYZlaebkF5PcS/K/Sf6510kk15AcIzk2MTGR4u2IiJwSNFJv13aESQusriH5UwDvcHnoy2Z2n8ePHQawyMxeJnkxgC0kLzCz1xtPNLONADYCwNDQkIW/dRGR5oUppcxCw7LAIG9mH476pGZ2DMCx2td7SP4SwB8DGIt8hyIiKchqa+FGqdTJkxwA8IqZTZF8F4DzADyTxrVERJqVhZF6kLgllFeQfBHA+wFsI7m99tAHATxOchzAfwH4vJm9Eu9WRUQkqlgjeTO7F8C9LsfvBnB3nOcWEZH4tOJVRCTDFORFRDJMQV5EJMNo1jml6SQnADzf7vsIcBaA37b7Jtqkl1870Nuvv5dfO9D5r/+PzGzA7YGOCvLdgOSYmQ0Fn5k9vfzagd5+/b382oHufv1K14iIZJiCvIhIhinIR7ex3TfQRr382oHefv29/NqBLn79ysmLiGSYRvIiIhmmIC8ikmEK8iF57Wdbe2w9yadJHiK5ql332AokbyJZqtu/96Ptvqe0kbys9rt9muRIu++n1Ug+R3J/7fed6XbhJG8neYTkE3XH3kbyYZK/qP09v533GJWCfHiu+9mSfA+AqwFcAOAyAN8hmZv945lyW93+vQ+0+2bSVPtdfhvARwC8B8Bnar/zXrOi9vvuylrxCH6A6v/H9UYAPGJm5wF4pPZ911CQD8lnP9vLAdxlZsfM7FkATwO4pLV3Jym6BMDTZvaMmR0HcBeqv3PJIDN7FEBjW/TLAdxR+/oOAMMtvamYFOTjKwL4Vd33L9aOZdl1JB+vfbTtqo+uTejF328jA/AQyT0k17T7ZtpggZkdrn39GwAL2nkzUaWyM1S3anI/28zx++8A4LsAvobq//hfA3ArgL9r3d1JG3zAzEok/xDAwyQP1ka8PcfMjGRX1Z0ryNdpZj9bACUA59R9/87asa4V9r8Dye8B+EnKt9Numfv9RmVmpdrfR0jei2oKq5eC/EskF5rZYZILARxp9w1FoXRNfFsBXE1yLsnFqO5n+/M231Nqav/IHVegOiGdZY8BOI/kYpKnoTrJvrXN99QyJE8n+VbnawCXIvu/80ZbAVxT+/oaAF31qV4j+ZBIXgHg3wAMoLqf7biZrTKzAyQ3A3gSwAkAXzCzqXbea8q+QXIpquma5wB8rr23ky4zO0HyOgDbAeQA3G5mB9p8W620AMC9JIFqvLjTzB5s7y2lh+SPACwHcFZt/+obAYwC2EzyWlRboV/ZvjuMTm0NREQyTOkaEZEMU5AXEckwBXkRkQxTkBcRyTAFeRGRDFOQFxHJMAV5EZEM+38OQEaJCXzw2AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne9UvrY7cIol"
      },
      "source": [
        "we will be trying to solve for `(m, b)` and can start with some arbitrary values, which are clearly wrong"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dJN3zg-APQx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "ee72b947-88e6-4038-9ebc-3a2d2a97d9c7"
      },
      "source": [
        "m_model = 1.0\n",
        "b_model = 0.0\n",
        "\n",
        "y_pred = y(x, m_model, b_model)\n",
        "plt.plot(x, y_true, 'o')\n",
        "plt.plot(x, y_pred, 'o')\n",
        "plt.vlines(0, -12, 12)\n",
        "plt.hlines(0, -12, 12)\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5sJH5oAHrSq"
      },
      "source": [
        "we'll start by using `grad` for calculating an update to our model params `(m, b)` with respect to a loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFZinhp0Ak9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45bf4287-e2c5-4351-dabe-ea688d5d76cc"
      },
      "source": [
        "from jax import grad, jit\n",
        "\n",
        "def sqr_loss(y_true, y_pred):\n",
        "  return jnp.mean((y_true-y_pred)**2)\n",
        "\n",
        "def loss_wrt_params(x, m, b, y_true):\n",
        "  y_pred = y(x, m, b)\n",
        "  return sqr_loss(y_true, y_pred)\n",
        "\n",
        "# note: by dft grad calculates gradients w.r.t the first arg\n",
        "#  but in this case we want it w.r.t. m & b, i.e. args 1 & 2\n",
        "gradient_wrt_loss = jit(grad(loss_wrt_params, argnums=(1, 2)))\n",
        "\n",
        "m_grad, b_grad = gradient_wrt_loss(x, m_true, b_true, y_true)\n",
        "print(\"for true: m_grad\", m_grad , \"b_grad\", b_grad)\n",
        "\n",
        "m_grad, b_grad = gradient_wrt_loss(x, m_model, b_model, y_true)\n",
        "print(\"for model params: m_grad\", m_grad , \"b_grad\", b_grad)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for true: m_grad -2.203858 b_grad 0.38934892\n",
            "for model params: m_grad 167.08748 b_grad -3.610651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yaz3COkbd3s9"
      },
      "source": [
        "we can use this gradient to run a trivial learning loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSN8twa5DB5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a24f71-6ebc-4d9f-ebea-06a271d28207"
      },
      "source": [
        "m, b = 1.0, 0.0\n",
        "learning_rate = 0.01\n",
        "\n",
        "print(\"true\", m_true, b_true)\n",
        "for _ in range(30):\n",
        "  m_grad, b_grad = gradient_wrt_loss(x, m_model, b_model, y_true)\n",
        "  m_model -= learning_rate * m_grad \n",
        "  b_model -= learning_rate * b_grad \n",
        "  print(\"grads\", m_grad, b_grad , \"=> model\", m_model, b_model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true -1.5 2.0\n",
            "grads 167.08748 -3.610651 => model -0.6708747 0.03610651\n",
            "grads 53.941635 -3.5384378 => model -1.210291 0.071490884\n",
            "grads 17.41423 -3.4676695 => model -1.3844333 0.10616758\n",
            "grads 5.6219215 -3.398316 => model -1.4406525 0.14015074\n",
            "grads 1.8149496 -3.3303494 => model -1.458802 0.17345424\n",
            "grads 0.58592945 -3.2637424 => model -1.4646612 0.20609167\n",
            "grads 0.18916094 -3.1984677 => model -1.4665529 0.23807634\n",
            "grads 0.061067313 -3.1344984 => model -1.4671636 0.26942134\n",
            "grads 0.01971218 -3.0718083 => model -1.4673607 0.30013943\n",
            "grads 0.0063598454 -3.0103722 => model -1.4674244 0.33024314\n",
            "grads 0.002050072 -2.9501648 => model -1.4674449 0.3597448\n",
            "grads 0.0006609708 -2.8911614 => model -1.4674515 0.3886564\n",
            "grads 0.00021739304 -2.8333385 => model -1.4674536 0.4169898\n",
            "grads 7.161498e-05 -2.7766714 => model -1.4674543 0.4447565\n",
            "grads 2.3335218e-05 -2.721138 => model -1.4674546 0.47196788\n",
            "grads 7.1674585e-06 -2.6667156 => model -1.4674547 0.49863502\n",
            "grads -5.066395e-07 -2.613381 => model -1.4674547 0.5247688\n",
            "grads -1.5348196e-06 -2.5611134 => model -1.4674547 0.55038\n",
            "grads -8.046627e-07 -2.509891 => model -1.4674547 0.5754789\n",
            "grads -5.736947e-07 -2.4596932 => model -1.4674547 0.60007584\n",
            "grads -7.599592e-07 -2.4104996 => model -1.4674547 0.62418085\n",
            "grads -1.0877848e-06 -2.3622894 => model -1.4674547 0.6478037\n",
            "grads -5.2154064e-07 -2.3150434 => model -1.4674547 0.67095417\n",
            "grads -4.991889e-07 -2.2687426 => model -1.4674547 0.6936416\n",
            "grads -9.164214e-07 -2.223368 => model -1.4674547 0.71587527\n",
            "grads -9.164214e-07 -2.1789007 => model -1.4674547 0.7376643\n",
            "grads -9.0897083e-07 -2.1353226 => model -1.4674547 0.7590175\n",
            "grads -9.313226e-07 -2.092616 => model -1.4674547 0.7799437\n",
            "grads -7.599592e-07 -2.0507636 => model -1.4674547 0.80045134\n",
            "grads -7.748604e-07 -2.0097485 => model -1.4674547 0.82054883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar4lT38IBGzj"
      },
      "source": [
        "## pytrees\n",
        "\n",
        "before we start though we should know that most models have more than 2 params. another thing we can do then is make use of the fact that the fundamental jax operators can work with dictionaries of params, called [pytrees](https://jax.readthedocs.io/en/latest/pytrees.html). this allows us to work with a single `params` object rather than clumsily having to manage single parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdjqBuWnBUaU"
      },
      "source": [
        "def y(params, x):\n",
        "  return params['m'] * x + params['b']\n",
        "\n",
        "def mean_sqrd_loss(params, x, y_true):\n",
        "  y_pred = y(params, x)\n",
        "  loss = (y_pred - y_true) ** 2\n",
        "  return jnp.mean(loss)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCoWRC0n3JYL"
      },
      "source": [
        "note how transforms like `grad` are pytree aware and it return gradients in the same structure as the params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgRgcL4CwTWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac84632-dfcd-4cae-fde1-2c5153227d20"
      },
      "source": [
        "true_params = {'m': m_true, 'b': b_true}\n",
        "model_params = {'m': 1.0, 'b': 0.0}\n",
        "\n",
        "# recall signature: mean_sqrd_loss(params, x, y_true)\n",
        "# note: grad takes gradients with respect to first arg\n",
        "grads = jit(grad(mean_sqrd_loss))\n",
        "\n",
        "print(\"grads w.r.t true_params\", grads(true_params, x, y_true))\n",
        "print(\"grads w.r.t model_params\", grads(model_params, x, y_true))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grads w.r.t true_params {'b': DeviceArray(0.38934892, dtype=float32, weak_type=True), 'm': DeviceArray(-2.203858, dtype=float32, weak_type=True)}\n",
            "grads w.r.t model_params {'b': DeviceArray(-3.610651, dtype=float32, weak_type=True), 'm': DeviceArray(167.08748, dtype=float32, weak_type=True)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auRIYnYXZW3I"
      },
      "source": [
        "##pmap\n",
        "\n",
        "let's run `pmap` to calc gradients in parallel. we'll reshape the input `(x, y)` data so we can `pmap` it across the 8 devices. we won't `pmap` across the model params `m` or `b`. this is effectively running a data parallel approach where each device runs the same model, but on 1/8th of the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U75uXTIAYxaS"
      },
      "source": [
        "# reshape x and y_true to 8 sets of 16 examples\n",
        "shardsize = datasize//num_replicas\n",
        "x = x.reshape((num_replicas, shardsize))\n",
        "y_true = y_true.reshape((num_replicas, shardsize))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbvGd_gn7vhp"
      },
      "source": [
        "if we pmap over the gradient calc directly what we end up getting is the 8 sets of gradients, each of which corresponds to the 16 examples that were provided. note how this represents the fundamental data parallelism approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcgbSkCv7n6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a336eb-1d03-4dae-a4e8-facacb7e9af7"
      },
      "source": [
        "from jax import pmap\n",
        "\n",
        "# recall signature: mean_sqrd_loss(params, x, y_true)\n",
        "p_grads = pmap(grad(mean_sqrd_loss), in_axes=(None, 0, 0))\n",
        "p_grads(model_params, x, y_true)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b': ShardedDeviceArray([-4.935777, -2.285525], dtype=float32, weak_type=True),\n",
              " 'm': ShardedDeviceArray([163.16254, 171.01242], dtype=float32, weak_type=True)}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtuu-X6UIq0P"
      },
      "source": [
        "at this point we have the gradients for each of the 8 batches of 16. we can sum these gradients and use them for an update."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMs1WBAkI5K_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac75898-7187-4de4-f527-b95d934b6038"
      },
      "source": [
        "g = p_grads(model_params, x, y_true)\n",
        "g = {k: jnp.sum(v) for k, v in g.items()}\n",
        "g"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b': DeviceArray(-7.221302, dtype=float32),\n",
              " 'm': DeviceArray(334.17496, dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5g5FfJXJNYJ"
      },
      "source": [
        "this kind of application of a function (like sum) to elements of a dictionary is pretty common when you represent params in dictionaries. to support operations on these jax provides a selection of `tree_utils` including `tree_map`. tree_map (and friends) can handle all kinds of potentially nested structures and run their operation on the leafs of the dict structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgRAZNR5JwPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74b5ce8-6109-409b-d842-a32c7a874466"
      },
      "source": [
        "from jax.tree_util import tree_map \n",
        "\n",
        "g = p_grads(model_params, x, y_true)\n",
        "g = tree_map(jnp.sum, g)\n",
        "g"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b': DeviceArray(-7.221302, dtype=float32),\n",
              " 'm': DeviceArray(334.17496, dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faOX8MgbJM6O"
      },
      "source": [
        "turns out this sum across all the 8 devices is something we can also do _inside_ the `pmap` call, not just after it. so far we've only used pmap to do independent calculations but there are a number of parallel ops we can call that operate _across_ the devices. in this case we can use `psum`. \n",
        "\n",
        "note how since `pmap` always returns a sharded value based on the #devices it broadcasts the gradients out to a replicated (8, 1) array. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYbFKCcPFtjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49055bbe-0f2c-4a34-c735-be169fd1b61d"
      },
      "source": [
        "from jax.lax import psum\n",
        "\n",
        "def p_grads(params, x, y_true):\n",
        "  grads = jit(grad(mean_sqrd_loss))(params, x, y_true)\n",
        "  grads = tree_map(lambda v: psum(v, 'device'), grads)\n",
        "  return grads\n",
        "\n",
        "p_grads = pmap(p_grads, in_axes=(None, 0, 0), axis_name='device')\n",
        "\n",
        "model_params = {'m': 1.0, 'b': 0.0}\n",
        "p_grads(model_params, x, y_true)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b': ShardedDeviceArray([-7.221302, -7.221302], dtype=float32),\n",
              " 'm': ShardedDeviceArray([334.17496, 334.17496], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAAamoVCI1JN"
      },
      "source": [
        "we've done the forward pass, the loss calc and now gradients all in parallel. what about the actual model update? we can do that in the `pmap` call too! we use the `tree_multimap` helper to update the params (a pytree) from the gradients (another pytree)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v0N3huqMWxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f1a4b9-99eb-436a-ea89-cc4a2b622a2f"
      },
      "source": [
        "from jax.tree_util import tree_multimap\n",
        "\n",
        "def p_update(params, x, y_true):\n",
        "  # calculate gradients summing across all devices\n",
        "  grads = jit(grad(mean_sqrd_loss))(params, x, y_true)\n",
        "  grads = tree_map(lambda v: psum(v, 'device'), grads)  \n",
        "  # apply update\n",
        "  def update(p, g):  \n",
        "    learning_rate = 0.001\n",
        "    return p - learning_rate * g\n",
        "  new_params = tree_multimap(update, params, grads)\n",
        "  # return new params\n",
        "  return new_params\n",
        "\n",
        "p_update = pmap(p_update, in_axes=(None, 0, 0), axis_name='device')\n",
        "\n",
        "model_params = {'m': 1.0, 'b': 0.0}\n",
        "p_update(model_params, x, y_true)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py:189: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
            "  'instead as a drop-in replacement.', FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b': ShardedDeviceArray([0.0072213, 0.0072213], dtype=float32),\n",
              " 'm': ShardedDeviceArray([0.665825, 0.665825], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeBq8a6mNzer"
      },
      "source": [
        "again notice how in the return values the params are being promoted from scalars to sharded arrays . if we want to include this in some kind of `params = update(params)` kind of loop we need to also _start_ with a sharded set. we can do this by replicating the single model params across all the devices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exk_R0uBM0MU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239407b1-2d30-4f55-a2bb-5a406bc8af32"
      },
      "source": [
        "def shard(x):\n",
        "  return pmap(lambda v: v)(x)\n",
        "\n",
        "def replicate(x, replicas=num_replicas):  \n",
        "  replicated = jnp.stack([x] * replicas)\n",
        "  return shard(replicated)\n",
        "\n",
        "model_params = {'m': 1.0, 'b': 0.0}\n",
        "model_params = tree_map(replicate, model_params)\n",
        "model_params"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b': ShardedDeviceArray([0., 0.], dtype=float32, weak_type=True),\n",
              " 'm': ShardedDeviceArray([1., 1.], dtype=float32, weak_type=True)}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZIYAajm4Yv6"
      },
      "source": [
        "and, while we're sharding, we might as well explicitly shard x and y too which has the nice side effect of replicating across the devices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVbER2R74XGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cb75754-06a9-468a-e5ac-9230f3fa7b74"
      },
      "source": [
        "x = shard(x)\n",
        "y_true = shard(y_true)\n",
        "  \n",
        "x.shape, type(x)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 64), jaxlib.xla_extension.pmap_lib.ShardedDeviceArray)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF9KDsyH4tPa"
      },
      "source": [
        "we now have everything we need to run this in across the eight devices in an as parallel way as possible\n",
        "\n",
        "* the model params, x and y, are replicated across all 8 devices. i.e. all 8 devices have a copy of the same params.\n",
        "* the data is also sharded aross the 8 devices. i.e. each device has 1/8th of the original data\n",
        "* the forward pass, loss, gradient calc & param update is done in parallel with a minimal combine step for gradients.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve68pTe81hgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84621106-79c5-4691-d66f-fdf779d58d63"
      },
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "def p_update(params, x, y_true):\n",
        "  # calculate gradients\n",
        "  loss, grads = jit(value_and_grad(mean_sqrd_loss))(params, x, y_true)\n",
        "  # do all reduce sum across all replicas\n",
        "  grads = tree_map(lambda v: psum(v, 'device'), grads)  \n",
        "  # apply update\n",
        "  def update(p, g):  \n",
        "    learning_rate = 0.001\n",
        "    return p - learning_rate * g\n",
        "  new_params = tree_multimap(update, params, grads)\n",
        "  return new_params, loss\n",
        "\n",
        "# note: we pmap across _all_ args now; model_params, x and y\n",
        "p_update = pmap(p_update, in_axes=(0, 0, 0), axis_name='device')\n",
        "\n",
        "for i in range(20):\n",
        "  model_params, loss = p_update(model_params, x, y_true)  \n",
        "  print(\"step\", i , \"loss\", jnp.mean(loss), \n",
        "        \"m\", model_params['m'][0] , \"b\", model_params['b'][0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py:189: FutureWarning: jax.tree_util.tree_multimap() is deprecated. Please use jax.tree_util.tree_map() instead as a drop-in replacement.\n",
            "  'instead as a drop-in replacement.', FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 loss 212.40942 m 0.665825 b 0.0072213025\n",
            "step 1 loss 160.328 m 0.37690836 b 0.014413719\n",
            "step 2 loss 121.39202 m 0.1271206 b 0.021577366\n",
            "step 3 loss 92.282 m -0.08883763 b 0.028712358\n",
            "step 4 loss 70.5167 m -0.27554798 b 0.03581881\n",
            "step 5 loss 54.241447 m -0.43697158 b 0.042896837\n",
            "step 6 loss 42.069927 m -0.5765331 b 0.049946554\n",
            "step 7 loss 32.965893 m -0.6971933 b 0.05696807\n",
            "step 8 loss 26.154783 m -0.8015122 b 0.0639615\n",
            "step 9 loss 21.057625 m -0.89170283 b 0.07092696\n",
            "step 10 loss 17.241653 m -0.9696787 b 0.07786455\n",
            "step 11 loss 14.383387 m -1.037094 b 0.0847744\n",
            "step 12 loss 12.241029 m -1.0953791 b 0.0916566\n",
            "step 13 loss 10.63384 m -1.1457704 b 0.09851128\n",
            "step 14 loss 9.426725 m -1.1893371 b 0.105338536\n",
            "step 15 loss 8.518703 m -1.2270035 b 0.11213849\n",
            "step 16 loss 7.8342905 m -1.2595685 b 0.11891124\n",
            "step 17 loss 7.317067 m -1.2877232 b 0.12565689\n",
            "step 18 loss 6.9248567 m -1.3120648 b 0.13237557\n",
            "step 19 loss 6.626137 m -1.3331097 b 0.13906737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJu_aPVahZ2m"
      },
      "source": [
        "what a way to solve `y=mx+b` !!"
      ]
    }
  ]
}