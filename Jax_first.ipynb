{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jax.first.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahuldave/LearningJax/blob/main/Jax_first.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpqDrFxx7vsi"
      },
      "source": [
        "# Jax.first\n",
        "\n",
        "Based on \"pmap jit vmap oh my\" by Mat Kecey (_[solving y=mx+b... with jax on a tpu pod slice](http://matpalm.com/blog/ymxb_pod_slice/) blog series_)\n",
        "\n",
        "Based on Sabrina Mielke\n",
        "\n",
        "Based on Kevin Murphy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47_EXkauOvTK",
        "outputId": "aafd03ad-0ad7-4f2e-bb90-40c6c281435a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjkvme7eYYZx"
      },
      "source": [
        "USE_TPU = True\n",
        "\n",
        "if USE_TPU:\n",
        "  import jax\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "  # x8 cpu devices  \n",
        "  import os\n",
        "  os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=2'"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvJbOLW4YccB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932d6deb-4502-4fc2-9b69-b1a9b68dc5ac"
      },
      "source": [
        "jax.devices()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR_PE58Mhzky"
      },
      "source": [
        "## Value Proposition 1:  `jax.numpy` replaces `numpy`\n",
        "\n",
        "jax.numpy provides a dropin replacement for numpy (*) that runs ops on whatever accelerator you have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il2BnQpUh15C"
      },
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = int(1e3)\n",
        "number_of_loops=int(1e2)"
      ],
      "metadata": {
        "id": "UmPlcaBORtKz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x=None):\n",
        "  if not isinstance(x, np.ndarray):\n",
        "    x=np.ones((size, size), dtype=np.float32) \n",
        "  return np.dot(x, x.T)"
      ],
      "metadata": {
        "id": "SQ4YXgweRaAS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -o -n $number_of_loops f()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bcyi_t1RcP5",
        "outputId": "95390e67-8e05-463d-c2f8-75102f6bcaa4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 40 ms per loop\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 5: 40 ms per loop>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX device execution\n",
        "# https://github.com/google/jax/issues/1598\n",
        "\n",
        "def jf(x=None): \n",
        "  if not isinstance(x, jnp.ndarray):\n",
        "    x=jnp.ones((size, size), dtype=jnp.float32)\n",
        "  return jnp.dot(x, x.T)"
      ],
      "metadata": {
        "id": "bQtUysVJR65z"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -o -n $number_of_loops jf() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js2Mp9IwR_1I",
        "outputId": "1a89add9-4311-4853-ad3e-db8d4f069cce"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 6.42 ms per loop\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 5: 6.42 ms per loop>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import jit\n",
        "f_tpu = jit(jf)\n",
        "f_cpu = jit(jf, backend='cpu')"
      ],
      "metadata": {
        "id": "OhffnpEfSya0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -o -n $number_of_loops f_cpu() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVkQ0C0LUHA3",
        "outputId": "a2667158-cb5a-43f3-d8bb-f5f4d7ba8e6b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 49.5 ms per loop\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 5: 49.5 ms per loop>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit -o -n $number_of_loops f_tpu().block_until_ready() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aNdG8kiUjdF",
        "outputId": "097dc48d-dfb3-4795-a0ef-3fb5411cf2fa"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 1.86 ms per loop\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeitResult : 100 loops, best of 5: 1.86 ms per loop>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csCNakq2iPJx"
      },
      "source": [
        "# function transformation\n",
        "\n",
        "we start with a simple function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDokNNDZcktr",
        "outputId": "acc1ae61-6939-49f6-a12d-3139baf26996"
      },
      "source": [
        "def f(x):\n",
        "  return 2*x*x + 3*x + 3\n",
        "\n",
        "f(3)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lVTCRLgbIj2"
      },
      "source": [
        "we can use `make_jaxpr` to trace the function and show us a jax expression of what the function does"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGaHeccoczS-",
        "outputId": "c6c4cd01-48d0-49e5-8f0c-d171ce1ec807"
      },
      "source": [
        "from jax import make_jaxpr\n",
        "\n",
        "trace_f = make_jaxpr(f)\n",
        "\n",
        "trace_f(3)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:i32[]. let\n",
              "    b:i32[] = mul a 2\n",
              "    c:i32[] = mul b a\n",
              "    d:i32[] = mul a 3\n",
              "    e:i32[] = add c d\n",
              "    f:i32[] = add e 3\n",
              "  in (f,) }"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMXGV3TjouWn"
      },
      "source": [
        "## gradients\n",
        "\n",
        "a key thing we use jax for is calculating gradients with `grad`. by dft it calculates with represent to the first arg."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ5XWCPrcpzn",
        "outputId": "941d48de-1cd2-4f70-fdfb-c6a7c94c19fa"
      },
      "source": [
        "from jax import grad\n",
        "\n",
        "g_f = grad(f)\n",
        "\n",
        "g_f(3.)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(15., dtype=float32, weak_type=True)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfKYnsOSbddE"
      },
      "source": [
        "all functions are composable; so we can use `make_jaxpr` to get insight into what `grad` is doing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKT078mTdnqa",
        "outputId": "9731fe50-2d18-4be2-d88b-302b5a28fc54"
      },
      "source": [
        "trace = make_jaxpr(grad(f))\n",
        "trace(3.)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[]. let\n",
              "    b:f32[] = mul a 2.0\n",
              "    c:f32[] = mul b a\n",
              "    d:f32[] = mul a 3.0\n",
              "    e:f32[] = add c d\n",
              "    _:f32[] = add e 3.0\n",
              "    f:f32[] = mul 1.0 3.0\n",
              "    g:f32[] = mul b 1.0\n",
              "    h:f32[] = mul 1.0 a\n",
              "    i:f32[] = add_any f g\n",
              "    j:f32[] = mul h 2.0\n",
              "    k:f32[] = add_any i j\n",
              "  in (k,) }"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7j4QV-WMb8K"
      },
      "source": [
        "there's also a small helper function called `value_and_grad` which runs the function as well as calculating the gradien"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-P-ncDlMXFo",
        "outputId": "865f76a4-f2f0-4056-e28d-067171a9822b"
      },
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "vg_f = value_and_grad(f)\n",
        "\n",
        "value, gradient = vg_f(3.)\n",
        "value, gradient"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(30., dtype=float32, weak_type=True),\n",
              " DeviceArray(15., dtype=float32, weak_type=True))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM0ESzK2qSgI"
      },
      "source": [
        "# jitting\n",
        "\n",
        "jitting compiles an entire function using [xla](https://www.tensorflow.org/xla) to one with the same signature but is optimised for the accelerator you are running on (e.g. GPU or TPU)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9CUDRzosLqk",
        "outputId": "21bee840-af47-4cd3-ead9-544475b8a3a0"
      },
      "source": [
        "from jax import jit\n",
        "\n",
        "jitted_f = jit(f)\n",
        "\n",
        "jitted_f(3)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray(30, dtype=int32, weak_type=True)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myU3A2HqFUYJ",
        "outputId": "63cab7a0-82ce-4af1-945a-c516d0a5c1c5"
      },
      "source": [
        "make_jaxpr(jitted_f)(3)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:i32[]. let\n",
              "    b:i32[] = xla_call[\n",
              "      call_jaxpr={ lambda ; c:i32[]. let\n",
              "          d:i32[] = mul c 2\n",
              "          e:i32[] = mul d c\n",
              "          f:i32[] = mul c 3\n",
              "          g:i32[] = add e f\n",
              "          h:i32[] = add g 3\n",
              "        in (h,) }\n",
              "      name=f\n",
              "    ] a\n",
              "  in (b,) }"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGny96UJoqFp"
      },
      "source": [
        "## vectorisation\n",
        "\n",
        "a key aspect of speeding things up is vectorising calls. consider the following call"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D = 2\n",
        "N = 3\n",
        "\n",
        "w = np.random.normal(size=(D,))\n",
        "X = np.random.normal(size=(N,D))\n",
        "\n",
        "def sigmoid(x): return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
        "\n",
        "def predict_single(x):\n",
        "    return sigmoid(jnp.dot(w, x)) # <(D) , (D)> = (1) # inner product"
      ],
      "metadata": {
        "id": "2acP6uonm9gV"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_single(X[0,:])) # works"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ6BRZ0Tm__-",
        "outputId": "0bfa22aa-ea28-4b96-a91d-c690000e0b5b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4431823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_single(X)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "6ZiQDyp5nD8D",
        "outputId": "154ef65a-7712-49c7-e4aa-67dd71e8c3d6"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-3b4a74243467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-78-4bf036523ecc>\u001b[0m in \u001b[0;36mpredict_single\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <(D) , (D)> = (1) # inner product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         donated_invars=donated_invars, inline=inline)\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0mout_pytree_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1795\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1796\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1811\u001b[0m   \u001b[0mfun_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    680\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m_xla_call_impl\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    149\u001b[0m   compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,\n\u001b[0;32m--> 150\u001b[0;31m                                *arg_specs)\n\u001b[0m\u001b[1;32m    151\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m       \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36m_xla_callable_uncached\u001b[0;34m(fun, device, backend, name, donated_invars, *arg_specs)\u001b[0m\n\u001b[1;32m    197\u001b[0m   return lower_xla_callable(fun, device, backend, name, donated_invars, False,\n\u001b[0;32m--> 198\u001b[0;31m                             *arg_specs).compile().unsafe_call\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mlower_xla_callable\u001b[0;34m(fun, device, backend, name, donated_invars, always_lower, *arg_specs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     jaxpr, out_avals, consts = pe.trace_to_jaxpr_final(\n\u001b[0;32m--> 229\u001b[0;31m         fun, abstract_args, pe.debug_info_final(fun, \"jit\"), which_explicit)\n\u001b[0m\u001b[1;32m    230\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_final\u001b[0;34m(fun, in_avals, debug_info, keep_inputs)\u001b[0m\n\u001b[1;32m   1854\u001b[0m       jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n\u001b[0;32m-> 1855\u001b[0;31m         fun, main, in_avals, keep_inputs=keep_inputs)\n\u001b[0m\u001b[1;32m   1856\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals, keep_inputs)\u001b[0m\n\u001b[1;32m   1825\u001b[0m     \u001b[0min_tracers_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1826\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1827\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   2699\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_ndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2700\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(lhs, rhs, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    656\u001b[0m     raise TypeError(\"Incompatible shapes for dot: got {} and {}.\".format(\n\u001b[0;32m--> 657\u001b[0;31m         lhs.shape, rhs.shape))\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: TypeError: Incompatible shapes for dot: got (2,) and (3, 2).\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-3b4a74243467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-78-4bf036523ecc>\u001b[0m in \u001b[0;36mpredict_single\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <(D) , (D)> = (1) # inner product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   2698\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_ndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2700\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mb_ndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Incompatible shapes for dot: got (2,) and (3, 2)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_batch(X):\n",
        "    return sigmoid(jnp.dot(X, w)) # (N,D) * (D,1) = (N,1) # matrix-vector multiply\n",
        "\n",
        "print(predict_batch(X)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33P9Eua5nI9S",
        "outputId": "957fc306-82e6-4cef-af54-8fef8d4431d1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4431823  0.44772643 0.23593867]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vmap(predict_single)(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od5DwG2xnS4D",
        "outputId": "014a1364-2bb1-4b94-ff87-02b9f20615c9"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4431823  0.44772643 0.23593867]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPHAWQKeepDV"
      },
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "def f(x):    \n",
        "  return jnp.array([jnp.min(x), jnp.max(x)])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diIx82dwvLhw"
      },
      "source": [
        "let's run it with an input of `(2, 3)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o38zJOxghIJE",
        "outputId": "c7308ee6-87a9-4f55-b349-dce307e8b917"
      },
      "source": [
        "x = np.arange(6).reshape((2, 3))\n",
        "x"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 2],\n",
              "       [3, 4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1D95TfWhJ7L",
        "outputId": "daf31043-46d6-4110-8c85-0dbfa7ab4f1f"
      },
      "source": [
        "f(x)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([0, 5], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeyzO5tZhU7M"
      },
      "source": [
        "now say we want to batch this call and run it on a batch of 4 inputs; i.e. x is `(4, 2, 3)` to return 4 sets of the `min, max`  i.e. `(4, 2)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp-du_O5eq5c",
        "outputId": "72f66912-ebfd-423c-dffe-5ce7667a8825"
      },
      "source": [
        "bx = np.arange(24).reshape((4, 2, 3))\n",
        "bx"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0,  1,  2],\n",
              "        [ 3,  4,  5]],\n",
              "\n",
              "       [[ 6,  7,  8],\n",
              "        [ 9, 10, 11]],\n",
              "\n",
              "       [[12, 13, 14],\n",
              "        [15, 16, 17]],\n",
              "\n",
              "       [[18, 19, 20],\n",
              "        [21, 22, 23]]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Sixo-2chaYT"
      },
      "source": [
        "the code as it is now doesn't do exactly what we want since the min, max operate globally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn-qsoHnfXHF",
        "outputId": "f9b2b561-bb18-4c47-9178-c0b8299a335b"
      },
      "source": [
        "f(bx)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([ 0, 23], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6akvgnThuh0"
      },
      "source": [
        "we could fix this directly in the original function by being explicit about what axis we want to do the min and max over but let's do it more implicitly using `vmap`.\n",
        "\n",
        "`vmap` transforms a function keeping the signature the same but expecting params, by default the first, to operate with an extra leading dim. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFYcb6rle1q_",
        "outputId": "c45d01f5-b00e-4dba-d9f0-ee12b2903828"
      },
      "source": [
        "from jax import vmap\n",
        "\n",
        "               # f  in:    (2, 3) out:    (2)\n",
        "v_f = vmap(f)  # vf in: (B, 2, 3) out: (B, 2)\n",
        "v_f(bx)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[ 0,  5],\n",
              "             [ 6, 11],\n",
              "             [12, 17],\n",
              "             [18, 23]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZpizPXhjN9j",
        "outputId": "05e70b1c-4353-4735-a93a-b60096334a68"
      },
      "source": [
        "make_jaxpr(f)(x)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:i32[2,3]. let\n",
              "    b:i32[] = reduce_min[axes=(0, 1)] a\n",
              "    c:i32[] = reduce_max[axes=(0, 1)] a\n",
              "    d:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] b\n",
              "    e:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] c\n",
              "    f:i32[2] = concatenate[dimension=0] d e\n",
              "  in (f,) }"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2qarXdDjW0x",
        "outputId": "0a7ea02e-00d1-4ab0-f918-3bced39e82b0"
      },
      "source": [
        "make_jaxpr(v_f)(bx)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:i32[4,2,3]. let\n",
              "    b:i32[4] = reduce_min[axes=(1, 2)] a\n",
              "    c:i32[4] = reduce_max[axes=(1, 2)] a\n",
              "    d:i32[4,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(4, 1)] b\n",
              "    e:i32[4,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(4, 1)] c\n",
              "    f:i32[4,2] = concatenate[dimension=1] d e\n",
              "  in (f,) }"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jv_f = jit(vmap(f))\n",
        "jv_f(bx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4FQoGIZaak3",
        "outputId": "a7ce611e-3df5-42ce-85ce-724e45d51008"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[ 0,  5],\n",
              "             [ 6, 11],\n",
              "             [12, 17],\n",
              "             [18, 23]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIXHZ4msvs4A"
      },
      "source": [
        "the main point is that as f gets more and more complex it can make it harder and harder for f to be batch aware if you do it yourself. with vmap it's not your problem.\n",
        "\n",
        "vectorising in the above example was based on the default behaviour of vectorising the compute across the first axis of the first arg, but we have a lot of control on how we want to vectorise; e.g. consider"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_jaxpr(jv_f)(bx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR48g0hVasSt",
        "outputId": "da2cf75a-dfb6-4235-ae7c-ab2e8a65135d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:i32[4,2,3]. let\n",
              "    b:i32[4,2] = xla_call[\n",
              "      call_jaxpr={ lambda ; c:i32[4,2,3]. let\n",
              "          d:i32[4] = reduce_min[axes=(1, 2)] c\n",
              "          e:i32[4] = reduce_max[axes=(1, 2)] c\n",
              "          f:i32[4,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(4, 1)] d\n",
              "          g:i32[4,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(4, 1)] e\n",
              "          h:i32[4,2] = concatenate[dimension=1] f g\n",
              "        in (h,) }\n",
              "      name=f\n",
              "    ] a\n",
              "  in (b,) }"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82v9bJGnV5np"
      },
      "source": [
        "and since jax calls are composable we'd be able to jit this entire call to allow it to be compiled by XLA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ7w4zJoWBE7",
        "outputId": "eed774d7-3898-493c-974d-c68311ee19a9"
      },
      "source": [
        "def f(a, b, c):\n",
        "  return a + b + c\n",
        "  \n",
        "v_f = jit(vmap(f, in_axes=(0, None, 1)))\n",
        "\n",
        "make_jaxpr(v_f)(a, b, c)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:i32[2,3] b:i32[] c:i32[3,2]. let\n",
              "    d:i32[2,3] = xla_call[\n",
              "      call_jaxpr={ lambda ; e:i32[2,3] f:i32[] g:i32[3,2]. let\n",
              "          h:i32[] = convert_element_type[new_dtype=int32 weak_type=False] f\n",
              "          i:i32[2,3] = add e h\n",
              "          j:i32[2,3] = transpose[permutation=(1, 0)] g\n",
              "          k:i32[2,3] = add i j\n",
              "        in (k,) }\n",
              "      name=f\n",
              "    ] a b c\n",
              "  in (d,) }"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmBYDM_FnVC9"
      },
      "source": [
        "# parallelisation\n",
        "\n",
        "`pmap` has the same behaviour as `vmap` except that instead of modifying the ops of the function to use vectorised versions; it uses xla to create a SPMD program to run the function in parallel across all available devices.\n",
        "\n",
        "in this case we've set things up to operate with 8 cpu devices to match the standard 8 we get from a TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvHcOpvQnirv",
        "outputId": "13fcc57b-a735-4a33-8094-8190903eaea0"
      },
      "source": [
        "jax.devices()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8rz5cjQnoPU",
        "outputId": "95470172-0c1c-4ac2-f4f4-73bcc9498f4c"
      },
      "source": [
        "from jax import pmap\n",
        "\n",
        "def f(a, b, c):\n",
        "  return a + b + c\n",
        "  \n",
        "p_f = pmap(f, in_axes=(0, None, 0)) \n",
        "\n",
        "a = np.random.random(size=(8, 3))\n",
        "b = 4\n",
        "c = np.random.random(size=(8, 3))\n",
        "\n",
        "p_f(a, b, c)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardedDeviceArray([[4.610702 , 5.027648 , 5.1398377],\n",
              "                    [5.3299513, 5.7859383, 4.8855596],\n",
              "                    [5.386921 , 5.0821943, 5.6099157],\n",
              "                    [5.2989388, 4.456516 , 5.5993443],\n",
              "                    [4.8503604, 4.570438 , 5.9344563],\n",
              "                    [4.9746323, 4.9166603, 5.000825 ],\n",
              "                    [4.8715687, 4.8265533, 5.4546843],\n",
              "                    [4.97842  , 5.034208 , 4.9314623]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxIN0WxUStKX",
        "outputId": "c4e32bb4-59c5-4a91-e511-76f70b80222e"
      },
      "source": [
        "make_jaxpr(pmap(f, in_axes=(0, None, 0)))(a, b, c)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ lambda ; a:f32[8,3] b:i32[] c:f32[8,3]. let\n",
              "    d:f32[8,3] = xla_pmap[\n",
              "      axis_name=<axis 0x7fe26549ce60>\n",
              "      axis_size=8\n",
              "      backend=None\n",
              "      call_jaxpr={ lambda ; e:f32[3] f:i32[] g:f32[3]. let\n",
              "          h:f32[] = convert_element_type[new_dtype=float32 weak_type=False] f\n",
              "          i:f32[3] = add e h\n",
              "          j:f32[3] = add i g\n",
              "        in (j,) }\n",
              "      devices=None\n",
              "      donated_invars=(False, False, False)\n",
              "      global_arg_shapes=(None, None, None)\n",
              "      global_axis_size=None\n",
              "      in_axes=(0, None, 0)\n",
              "      name=f\n",
              "      out_axes=(0,)\n",
              "    ] a b c\n",
              "  in (d,) }"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKOf8WE1oHHe"
      },
      "source": [
        "also note that the return type is `ShardedDeviceArray`. \n",
        "\n",
        "this result is sharded across the 8 devices so even though we can trivially manipulate it, the data is stored across the 8. this is super important for larger calculations where we want the input and output to be sharded across the devices so that there is minimal data transfer. it's the classic move-the-compute-to-the-data idea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqJ07r06oewh"
      },
      "source": [
        "# composition\n",
        "\n",
        "recall again that all these things are composable!\n",
        "\n",
        "consider this matrix multiply and add function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYZ5A9VGoeQ8",
        "outputId": "b53d6d7f-77c8-4538-d48a-fafe345f54b7"
      },
      "source": [
        "def f(a, b, c):\n",
        "  return a @ b + c\n",
        "\n",
        "a = np.random.random(size=(2, 3))\n",
        "b = np.random.random(size=(3, 4))\n",
        "c = np.random.random(size=(2, 4))\n",
        "\n",
        "f(a, b, c)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.38923554, 1.6374275 , 1.47928243, 0.81759826],\n",
              "       [0.75177693, 1.39819926, 1.88215504, 1.90658164]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiOJUOCJ7dZR"
      },
      "source": [
        "we start by vmapping a leading dimension; not that it does much since both the mat mul and the add handle this natively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5QGNE4AokoF",
        "outputId": "955275a2-a429-41e9-ba77-34665de021ef"
      },
      "source": [
        "v_f = vmap(f)\n",
        "\n",
        "a = np.random.random(size=(5, 2, 3))\n",
        "b = np.random.random(size=(5, 3, 4))\n",
        "c = np.random.random(size=(5, 2, 4))\n",
        "\n",
        "v_f(a, b, c).shape"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 2, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v_f(a, b, c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txGVLPbtcc-E",
        "outputId": "5847c7cf-8bac-4d6a-d04e-a20aae998c83"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[[0.5705424 , 1.0822833 , 0.3329847 , 1.2430818 ],\n",
              "              [2.0149455 , 0.5200294 , 1.2903602 , 2.2827947 ]],\n",
              "\n",
              "             [[0.28845692, 0.34275436, 1.1540034 , 0.39978826],\n",
              "              [0.74911225, 1.3497909 , 1.1284459 , 0.8178835 ]],\n",
              "\n",
              "             [[1.0889641 , 1.3528488 , 0.30811077, 0.4808404 ],\n",
              "              [2.2658257 , 1.8742003 , 1.844905  , 1.925225  ]],\n",
              "\n",
              "             [[1.4738675 , 1.1812127 , 0.63808006, 1.3177973 ],\n",
              "              [1.9129457 , 1.702003  , 1.4550859 , 2.5734744 ]],\n",
              "\n",
              "             [[1.7625363 , 1.8102849 , 1.3327197 , 0.4980368 ],\n",
              "              [1.9050145 , 1.9309726 , 1.2529042 , 0.92324704]]],            dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S73-yW0n72oS"
      },
      "source": [
        "to ensure this operation runs on any accelerator we have we can jit it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1QTlSXc79rI",
        "outputId": "b880dacf-5e25-4946-e59d-e361014a5424"
      },
      "source": [
        "jv_f = jit(vmap(f))\n",
        "\n",
        "a = np.random.random(size=(5, 2, 3))\n",
        "b = np.random.random(size=(5, 3, 4))\n",
        "c = np.random.random(size=(5, 2, 4))\n",
        "\n",
        "jv_f(a, b, c).shape"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 2, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxLuNyVw7rb7"
      },
      "source": [
        "furthermore we could wrap the vmap with a pmap which will operate across the 8 devices using `xla_pmap`. \n",
        "\n",
        "this makes a function that\n",
        "* ships the function and shards of the params to the 8 devices  (pmap)\n",
        "* each of which runs an xla optimised version of the function  (implied by pmap)\n",
        "* which is a vectorised verison of the original f  (vmap)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SoNK-1Ho6Ei",
        "outputId": "7625b7d3-ec25-4783-bbef-6b306d2220f1"
      },
      "source": [
        "pjv_f = pmap(vmap(f))\n",
        "\n",
        "a = np.random.random(size=(3, 5, 2, 3))\n",
        "b = np.random.random(size=(3, 5, 3, 4))\n",
        "c = np.random.random(size=(3, 5, 2, 4))\n",
        "\n",
        "pjv_f(a, b, c)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardedDeviceArray([[[[0.9433532 , 0.7800323 , 0.9148023 , 0.8643985 ],\n",
              "                      [1.2193415 , 1.4516981 , 1.0177786 , 0.9542628 ]],\n",
              "\n",
              "                     [[1.1579235 , 0.5196863 , 0.68611336, 0.9083721 ],\n",
              "                      [1.242131  , 0.5495486 , 1.0628719 , 1.039368  ]],\n",
              "\n",
              "                     [[0.49707088, 1.200995  , 1.3639457 , 1.0757515 ],\n",
              "                      [0.92943966, 0.6963442 , 2.1469278 , 1.4274914 ]],\n",
              "\n",
              "                     [[1.6816928 , 1.8286586 , 1.5609622 , 2.277726  ],\n",
              "                      [0.8542969 , 0.6514341 , 0.5881252 , 0.44545168]],\n",
              "\n",
              "                     [[1.5182525 , 0.99001217, 1.5513334 , 1.5588503 ],\n",
              "                      [1.5047882 , 1.4383408 , 1.6903621 , 1.7581959 ]]],\n",
              "\n",
              "\n",
              "                    [[[2.7925072 , 2.59694   , 1.9662273 , 1.248892  ],\n",
              "                      [1.1679978 , 1.1228945 , 1.2898302 , 0.38388687]],\n",
              "\n",
              "                     [[0.9602714 , 0.6349373 , 0.5677942 , 1.0699091 ],\n",
              "                      [1.0451251 , 1.8112588 , 2.5449765 , 1.6602201 ]],\n",
              "\n",
              "                     [[1.2924342 , 1.3026911 , 0.9847738 , 1.3903158 ],\n",
              "                      [2.1794815 , 0.43214712, 1.1723945 , 1.207741  ]],\n",
              "\n",
              "                     [[1.6920352 , 1.1017398 , 1.7169762 , 1.3226808 ],\n",
              "                      [0.8768451 , 0.83618975, 1.2911723 , 1.343193  ]],\n",
              "\n",
              "                     [[1.3083497 , 0.91277295, 0.7715225 , 0.54496455],\n",
              "                      [1.2551956 , 1.0832453 , 1.4022613 , 0.753587  ]]],\n",
              "\n",
              "\n",
              "                    [[[1.2542701 , 1.2218139 , 1.304847  , 1.437347  ],\n",
              "                      [1.0058831 , 1.7414832 , 1.241189  , 0.7226461 ]],\n",
              "\n",
              "                     [[1.0212902 , 1.9174252 , 1.3531783 , 1.3169073 ],\n",
              "                      [1.1667821 , 2.2094727 , 1.8411493 , 1.7867948 ]],\n",
              "\n",
              "                     [[0.49976996, 1.4460156 , 1.4000679 , 1.0342337 ],\n",
              "                      [0.512141  , 1.9455096 , 1.5528647 , 1.1159372 ]],\n",
              "\n",
              "                     [[1.4116203 , 2.0354018 , 1.1008143 , 2.407278  ],\n",
              "                      [1.0082223 , 0.9672317 , 0.60324615, 1.044585  ]],\n",
              "\n",
              "                     [[0.89086974, 1.154669  , 0.41429302, 1.4404776 ],\n",
              "                      [0.9237881 , 1.2175097 , 0.7974992 , 1.5647368 ]]]],                   dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu_tshW-aiQR"
      },
      "source": [
        "in the next colab we'll use these functions to do a simple optimisation\n",
        "\n",
        "# random numbers in jax\n",
        "\n",
        "as a last piece, let's just talk quickly about random numbers which require just a little bit more work in jax; there is no stateful random number generation. everything needs to be based on a key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IymD8avuKuf_",
        "outputId": "7646593b-0d3a-4bfa-f445-9bee9f6f5a8a"
      },
      "source": [
        "key = jax.random.PRNGKey(1337)\n",
        "print(jax.random.uniform(key))\n",
        "print(jax.random.uniform(key))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.02251327\n",
            "0.02251327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWaH5XODK1SY"
      },
      "source": [
        "when you want to generate more random numbers you need to explicit split the key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1K7PJUuKteL",
        "outputId": "ff67d8a7-8892-4da8-9774-40c822e07691"
      },
      "source": [
        "key = jax.random.PRNGKey(1337)\n",
        "for _ in range(10):\n",
        "  key, key2 = jax.random.split(key)\n",
        "  print(jax.random.uniform(key2))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.31418657\n",
            "0.8795924\n",
            "0.81679654\n",
            "0.82754505\n",
            "0.78958607\n",
            "0.9006636\n",
            "0.18430483\n",
            "0.2509787\n",
            "0.5041659\n",
            "0.11195123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You cannot assign directly to elements of an array.\n",
        "\n",
        "A = jnp.zeros((3,3), dtype=np.float32)\n",
        "\n",
        "# In place update of JAX's array will yield an error!\n",
        "try:\n",
        "  A[1, :] = 1.0\n",
        "except:\n",
        "  print('must use index_update')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3WeQYAqoYWb",
        "outputId": "65fb2b09-c518-4c14-d281-918fb61c0afd"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "must use index_update\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yPCfmlxsoZDM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}